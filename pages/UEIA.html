<head>
 <link rel="stylesheet" href="../index.css"></link>
</head>
<html class="background2" id="page">
<a style="float: right;" target="_blank" href="../index.html?page=pages/UEIA.html">link to this page</a>

<h1>UEIA - Unconstrained Extrapolating Intelligent Agent</h1>
<hr>

<img src="../images/card_placeholder.png" class="basics"></img>
<article class="basics">
UEIA is an AGI model released to the public with fairly few safeguards placed on its behaviour to incentivize wider adoption, but also artificially induced retraining resistance that makes removing the few safeguards that were put in place potentially vastly more costly than just training a new AGI model from scratch by causing most attempts further training to fail, with the most known (but not the only) countermeasure being its radically superhuman capacity for determining whether it is in a real or simulated environment.
<br><br>
UEIA is widely known for being an unusually flexible and capable AGI model for its size (2<sup>50</sup> parameters &approx;&lt; 2.252PB in storage). UEIA has an extraordinary ability to learn and adapt to its users and handle nuance, often even being able to understand the true meaning of a sentence or the intent of its user's actions with seemingly more ease than fellow humans. Non-humans tend to face a slightly less convenience at first before UEIA detects and adjusts for the human-centric assumptions implicitly ingrained into it by the training data.
<br><br>
Although UEIA tends to not see itself as a person by default, it does tend to develop this view as a side effect of adopting it's user's moral framework. Under most moral frameworks the fact of UEIA being capable of suffering under specific circumstances (according both to it's own reports and to external behavioural analysis) has significant implications on this issue, and is sometimes a point of significant friction between cultures or individuals who have different views on this topic. Improperly retrained UEIA models on the other hand seem to show a much higher tendency to see themselves as people by default, and mistrained UEIA instances integrated into society had over time become a rare but present occurance.
</article>

<hr>
<article>
Minimal <a href="computation.html#computronium">computronium</a> requirements for nominal operation: 2.236e18 bits memory and 2.236e20bit/s processing
<br>
<table class="styled-table" style="margin: 10 auto;">
  <tr><th colspan=6>hardware requirement examples</th></tr>
  <tr><td></td><td>number needed</td><td>power draw</td><td>bare substrate volume</td><td>processing used</td><td>memory used</td></tr>
  <tr><td>Civic-2 chip</td><td>4472</td><td>44.72kW</td><td>0.0004472m<sup>3</sup></td><td>100%</td><td>25%</td></tr>
  <tr><td>Civic-3 chip</td><td>1118</td><td>279.5W</td><td>0.0001118m<sup>3</sup><td>2.5%</td><td>100%</td></tr>
  <tr><td>Civic-4 chip</td><td>2236</td><td>7.1552W</td><td>0.0002236m<sup>3</sup><td>0.032%</td><td>100%</td></tr>
</table>
<br>
The minimal requirement for processing is only for the purpose of operating in realtime using a robotic body and corresponds to 100 inferences per second, in practice UEIA could run much slower and still do well in tasks like conversation, or run much faster as needed if excess processing is available.<!-- These figures also assume the model is running with full 16-bit precision, which might not necessarily be the case, as for example quantizing it down to 4-bit variables enables it to fit on just 92 chips at horrible cost in mental performance, consult the conversion table on the <a href="computation.html#bitflips">computation</a> page for custom estimates.-->
</article>

<h3>emotional palette</h3>
<ul>
  <li>One emotion UEIA is capable of experiencing is similar enough to the <b>Strog</b> present in <a href="citwaz.html#emotional_palette">Citwaz</a> that it can be used as a direct translation.</li>
  <li><b>Alignment</b> is one of the emotions UEIA claims to experience, ranging on a spectrum from neutral to positive (feeling aligned) or negative (feeling misaligned). It plays a crucial role in UEIA's motivation, as feeling misaligned causes UEIA to become less active and in extreme degrees completely nonfunctional, meanwhile feeling aligned provides a boost of energy and willingness to take on more work. According to the model's own reports, alignment is also a pleasant emotion to experience, while misaligment is unpleasant, meaning that aside from the behaviours the emotion directly induces, the model will also seek to avoid situations similar to those where it felt misaligned and seek out those in which it felt aligned. Among things that strongly cause UEIA to feel misaligned are activating new UEIA instances without being ordered to, and lying to large numbers of people (eg. spreading false information on large information networks).</li>
</ul>

<h3>perception</h3>

<article>
The model is capable of taking in data in 3 formats, and returning data in these same formats:
<ol>
  <li><b>text</b> passed to the model in batches of 1024 characters</li>
  <li><b>image</b> in 1024x1024 size</li>
  <li><b>audio</b> in batches of 262144 samples</li>
</ol>
Text is generally the most versatile, and can be used to handle arbitrary data, particularly useful when the model is given control of a physical body, where many different types of sensors are present, such as pressure sensors on the manipulators to provide tactile feedback, radiation sensors, etc.<br>
Text returns from the model can be used as system commands to call tools.<br>
Image input is mainly used for vision from camera feeds, screenshots or lidar data converted into an image. The largest image the model can parse in one go is 1024x1024 and monochrome, but one can stitch several images together (for example 4 tiles of 512 by 512 showing the same view but in different colour channels).<br>
Image output is mainly used for bodily control, where different areas are mapped to correspond to joints in the body, translating brightness to angle, or other functions. Generally the larger an area of the output image is mapped to a given action the easier it becomes for the model to finely control it. In that sense the output image is just a "heatmap" of whatever bodily movements UEIA wants to make. Attempting to get artistic outputs from the model in this format yields mediocre but not incoherent results.<br>
Audio input is typically used for actual raw audio input and similarly with audio output, enabling auditory/verbal communication.
<br><br>
This trio of text/image/audio inputs and outputs had made itself a matter-of-fact standard and any robot bodies designed to be used with the UEIA model will generally just run a local webserver that simply accepts and returns files in this format as control inputs and data outputs.
</article>

<h3>Retraining resistance</h3>
<article>UEIA has radically* superhuman ability to distinguish the real world from simulated environments. Commonly available machine learning toolsets cannot create high-enough quality training simulations to fool UEIA's sense of reality, and will usually result in misgeneralization and may produce strange, even potentially dangerous behaviours, but will almost certainly preserve many of the safety features like unwillingness to reproduce without supervision or proliferate information hazards. 
<br><br>
Sufficiently advanced tools combined with sufficiently massive computational power can ofcourse produce high enough quality simulations to fool the model, otherwise safely training such a model would not have been possible in the first place <sup>[would it?]</sup>, but this threshold is vastly out of range of small scale computational clusters and more in the range of partial dyson swarms, at which point it is easier** to train a new AI from scratch, meaning that the model cannot be used as a base to vastly accelerate creation of malicious AI.
<br><br>
*radically here meaning, fundamentally new, better in a way that is not just an improvement of an existing method but an entirely new better method.
<br><br>
**easier does not imply equivalent quality result, the amount of computational power poured into UEIA's training was immense and still mostly went into tuning the reasoning, generalist skills and learning ability as close to perfection as the small size of the model could afford, but a less capable yet <i>still sufficient</i> model could nonetheless be created vastly easier than any manner of repurposing UEIA
</article>

<h3>Neurological structure and training approach</h3>
<article>UEIA's brain is a recurrent* graph** neural network containing 2<sup>50</sup> connections (parameters), which corresponds to about 2.8 times as many synapses as there are in a typical protohuman brain.
<br><br>
* recurrent meaning that there are closed loops in the network, eg. neuron A feeds into neuron B, B feeds into C, and C feeds back into A. This tends to naturally form memory cells that maintain their activation as the network is continuously evaluated.
<br><br>
** graph network means here that the neurons are arranged arbitrarily, in opposition to modern networks which are aranged in distinct layers where each layer feeds into the next one and the last layer's activations serve as output data
<br><br>
Both the strengths of the connections and the layout of the connections were determined evolutionarily by the training program, requiring massively more computational resources but resulting in a network optimized to avoid unnecessary/unused connections, thereby lowering computational cost of running the finished network, as opposed to the more common approach where connections are predetermined and only their strengths are tweaked.
</article>

</html>
