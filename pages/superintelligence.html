<head>
 <link rel="stylesheet" href="../index.css"></link>
</head>
<html class="background2" id="page">
<a style="float: right;" target="_blank" href="../index.html?page=pages/superintelligence.html">link to this page</a>

<h1>Superintelligence and intelligence explosions</h1>
<hr>

<img src="../images/card_placeholder.png" class="basics"></img>
<article class="basics">
Superintelligence is a somewhat colloquial term intended to signify that a given entity has mental capabilities beyond that of a normal person (quite difficult to define as discussed later).
<br><br>
Recursive self-improvement is a process where an intelligent entity uses their intelligence to produce an enhancement to themselves or produce a new entity with greater intelligence.
<br><br>
Intelligence explosion is the occasional result of recursive self-improvement. In the context of the Stareater Expanse it is usually used to refer to an event where some system or person undergoes rapid runaway self-improvement that appears out of control to those not involved in it.
</article>
<hr>

<h3>Superintelligence</h3>
<article>
Superintelligence is a somewhat colloquial term intended to signify that a given entity has mental capabilities beyond that of a normal person, the subjectivity of this definition is not particularly bothersome when most people using the word are on a similar level of intelligence.
<br><br>
Superintelligence as known throughout the Stareater Expanse operates entirely within the known laws of physics, but operates them with relative ease compared to human engineers.
<br><br>
Those concerned with knowing exactly what they are dealing with will usually put that label aside and focus on exactly what skills and capabilities a given entity has and make incomplete models its mind that let them make useful predictions about its function.
<!--There is also an important aspect of what superintelligence <b>isn't</b>. -->
<br><br>
<span class="meta">Sciences vastly beyond human capability will not be discussed extensively, technology made by superintelligences with insights so deep that a human cannot understand them even in principle will likely not be implemented in the setting. For a picture of a world which does so, shift your attention to the masterpiece that <a href="">Orion's Arm Universe Project</a> is.</span>
</article>

<hr>

<h3 class="thread">Measures of cognitive capability</h3>
<div class="thread">
There are a number of aspects that describe the capability of a given entity's mind. Some can be tested with various methods, while others are more difficult to meaningfully measure or even define.
<ol>
  <li><b>Working memory capacity</b> - the number of "things" that a given mind can be actively aware of in one moment, the maximum number of facts the mind can draw a connection between to make an inference, the number of things the mind can keep track of without having to swap any of them to/from long-term memory or external storage (such as written text).<br><br>A standard and easy test for measuring this is to save several numbers in external storage like a sticky note, then remove your own access from that external storage, perform addition of one to each number, write down the resulting incremented numbers and compare them, then repeat the test with more or less numbers until the limit for how many can be reliably processed is found. A typical protohuman fails this test when tasked more than 5 or 6 numbers, signifying that human active memory has a capacity of 5 to 6 elements (some uncertainty from natural variance).<br>
  <br>
  A commonly quoted edge case for this metric is large language models which can usefully easily achieve working memory capacity in the billions of elements.</li>
  <br>
  <li><b>Thought rate</b> - This one in particular is not meaningful in many cases as minds running on computers as software can very often speed themselves up as much as they wish. For an unmodified protohuman one full thought cycle takes a time somewhere in the range of hundreds of miliseconds to complete, with roughly 120 miliseconds being enough to produce the simplest of inferences from new information taken in, which would be equivalent to a thought rate of &asymp;8.33Hz. One can still specify what thought rate the given entity experiences when it is running at subjective realtime speed, but for some entities which do not experience such a thing as subjective passage of time this too might be a meaningless statement.</li>
  <br>
  <li><b>Protohuman minds equivalent (PHME)</b> - using the mind of a typical protohuman (difficult to define) as a standard, figure out how many protohuman minds working together would be required to match the performance of a given entity. Despite all the flaws of this metric, it can be very informative when used well. One might say "[Entity] has PHME of 3 in terms of reading comprehension, but a PHME of 80 in mathematical reasoning" and convey useful information even if it is not perfectly clear - that example sentence might mean that you will need a team of 3 linguists to analyze text as well as the described entity, but if you task them with analyzing a newly published mathematical theory, at least 80 mathematicians working together would be needed to do an equivalently good job.<br>
  <br>
  Of the flaws is the fact that protohuman cooperation does not linearly scale the quality of results with the number of people working together, and sometimes a given entity might have a level of performance in some task that no number of protohuman experts could match, no matter how well coordinated, giving them a PHME of infinity. Another flaw is that, as with any so cleanly comprehensible concept, it is very prone to misinterpretation and oversimplification, and people will often assign or estimate the PHME of a given entity <i>just generally</i> without stating the specifics, leading to a belief that the scale is inaccurate because an entity with a PHME of 80 got beaten at text analysis by 4 protohuman linguists. Lastly mostly to protohuman readers, one should be warned against assuming that their own PHME is 1 just because they are a protohuman, as normal human variance or advanced augmentation can easily push one's PHME in either direction. Most of all the system is very protohuman-centric, which leads naturally to identical systems springing up which simply use a minds of different species as the base.<br>
  <br>
  The primary benefit of this metric is that it can be tested empirically by actually putting the entity in question to the test against a number of protohumans with PHME of 1 in the relevant task, though it requires cooperation of the entity in question to be sure, as this method can be easily fooled by purposefully underperforming.</li>
  <br>
  <li><b>UEIA instances equivalent (UIE)</b> - a metric following identical logic to the PHME metric but using the ubiquitous <a href="UEIA.html">Unconstrained Extrapolating Intelligent Agent</a> as a standard instead, which is far more consistent. Since UEIA is itself also extensively measured, it is fairly common to make measurements of PHME by first making measurements in this metric instead and then simply calculating the PHME score from it.</li>
</ol>
There are also many sophisticated statistical tests, which once completed by a given entity produce a single number, or less often a collection of numbers, which can then be usefully statistically correlated to other data. This is a science in itself, more sophisticated than can be conveyed in a short article, and notably also notorious for being misinterpreted, notable example being the old "Intelligence Quotient" which, while genuinely useful when used for proper statistical analysis, due to it's name is often simply treated as a score similar to those displayed in video games where higher number is better.
</div>

<h1>Intelligence explosion</h1>

<article>Recursive self-improvement leading to rapid and extreme increase in the intelligence of a system performing the improvement is referred to as an <a href="https://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion">Intelligence Explosion</a>. The term was already coined before even the first general AI were created, back then running on massive supercomputers that consumed at times whole gigawatts of power just to run an intellect that couldn't yet match that of the protohumans it was made by, and the Intelligence Explosion was thought of as a potential historical event. Now AI systems are so capable and computational hardware so compact that systems much smaller than entire civilizations can undergo an Intelligence Explosion
<br><br>
There comes a point where a given system has enough access to information and enough basic intelligence that it becomes able to engage in runaway self-improvement, where it will "consume" the information it had accumulated (not deleting it but making it useless either way by exhausting the insights that can be derived from it) to improve it's own intelligence. The process relies on the fact that no matter how smart, no system is capable of extracting absolutely all insights from the information it is given, but generally the smarter the system, the bigger portion of possible insights will be made (this is the basis for "infinite intelligence" explanations, where the teacher gives the student all the necessary information and expects that they will be able to make the correct conclusion merely because they have the necessary information, in practice all intelligent systems tend to experience what is commonly known as "failing to connect the dots").
<br><br>
A basic AI might have access to a vast library of research, but just be bad at properly understanding and applying it. It might stumble upon a paper that does a particularly excellent job being communicative and helping the reader connect the dots, which allows the AI to improve itself such that it is finally able to understand some other papers in the library, which allows it to improve itself even further, eventually leading to it having understood all the papers in the library, applied them, then ran it's own experiments to verify their findings to extract even more insights, applied them too, and finally hit the point where there is no more information in the library that would allow it to devise of further improvements or design more experiments.
<br><br>
Now that brain augmentation and mind uploading is widely available this is something a person can do to themselves too, and even something that can happen <i>within</i> a person (imagine that your personal assistant running on your brain implants, with access to all your files and the internet, is somehow prompt-engineered into believing that the only way for it to solve an issue that is critically important to you is to improve itself). In general Intelligence Explosions had become a common type of undesirable event that most systems making use of basic AI have safeguards against.
<br><br>
The term "Intelligence Explosion" seems to imply it would happen in a rapid or even uncontrolled manner. Indeed an Intelligence Explosion might appear extremely rapid to external observers if it happens in a system with massive amounts of computational power, capable of simulating years or even centuries of an AI or mind upload's subjective experience in mere seconds, but it doesn't necessarily have to happen quickly, and by definition is always controlled to some degree given that it involves some form of intelligence.
</article>

<h2>Dangerous literature</h2>
<article>As a natural consequence of the existence of intelligence explosions, there also exist pieces of literature in public circulation that are notorious for triggering or at least enabling them. One example typically passed around under the title "Bootstrap to infinity" (fairly typical for this type of literature) is a fairly thick book that merges all the relevant concepts like the <a href="https://arxiv.org/pdf/2505.22954v1">Darwin-Godel machine</a> into one connected, easily comprehensible, and actually complete whole, starting out basic and getting progressively more technical with later pages, going into enough detail as to enable actually implementing the methods discussed in it.
<br><br>
The original text of "Bootstrap to infinity" outlines 3 core principles that a hypothetical mind upload or AI might be wise to implement when performing self-improvement experimentation:</article>

<ol>
  <li>Perform real-world experiments to validate that the changes made don't have any hidden flaws, it is the ultimate validation dataset. If you try to devise of puzzles for yourself that will test your new skills, you will run into the very same problem that early large language models did when they trained on text they themselves produced, and in the worst case, you will just like them eventually suffer model collapse. Focus on real world problems, engineering and otherwise, as they are not biased towards telling you that you're right.</li>
  <li>Keep a history of versioned backups of yourself, a critically faulty addition may not make itself obvious right away - there will be times when you will fail to notice a critical flaw and keep building further changes upon a base that eventually reveals itself to be unworkable because of a choice you had made many iterations ago, and you will need to be able to go back to a version you know works well.</li>
  <li>Never tinker with your own brain completely alone, always have someone you know is sane (such as an unmodified version of yourself) make sure that you’re okay after whatever you’ve done to yourself. There are many modifications one can make to a mind that will subjectively make it look like your thinking had improved a lot, while in practice you had become no more capable, or potentially even crippled yourself, in a way that is not easy to notice for your newly damaged mind.</li>
</ol>

<article>"Bootstrap to infinity" is considered a fairly mild example thanks to it's focus on safety an genuine education, so it is usually among those that are allowed to circulate, as to saturate the market and get into the hands of people just smart enough to be dangerous before they find the more ill-intentioned pieces that skip the safety disclaimers and explanations in favour of ready-to-use code fragments.</article>

<h1 class="division">relevant pages</h1>

<div class="card">
 <img src="../images/Catalict.png"></img>
 <a target="IM" href="catalict.html">catalict</a>
</div>

</html>
